{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to feature Engineering and Machine Learning with Apache Spark\n",
    "\n",
    "Welcome to our comprehensive tutorial on Data Engineering and Machine Learning using Apache Spark. In this interactive guide, we'll dive deep into the principles of data engineering and explore how to apply machine learning techniques to real-world data. Our focus will be on leveraging Apache Spark, a powerful open-source distributed computing system designed for fast computation.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- **Understand feature Engineering for time series data:** Learn the essentials of data engineering, including data loading, preprocessing, and feature engineering, using Spark's DataFrame API. Discover how to prepare data effectively for analytical or machine learning applications.\n",
    "\n",
    "- **Explore Machine Learning Pipelines:** Gain hands-on experience with building and evaluating machine learning models. Understand how to construct pipelines for streamlined data transformation and model training.\n",
    "\n",
    "- **Practical Applications:** Apply what you've learned to a dataset, performing feature engineering tasks and use machine learning models to solve real-world problems.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize Spark Session\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPaypark Data Engineering and ML Tutorial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./processed.pq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\context.py:384\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\context.py:331\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 331\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    111\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Paypark Data Engineering and ML Tutorial\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.parquet(\"./processed.pq\")\n",
    "# Display the first few rows to verify the data is loaded correctly\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+\n",
      "|                 ts|               uid|      source_ip|source_port|        dest_ip|dest_port|proto|service| duration|orig_bytes|resp_bytes|conn_state|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|    label|      detailed-label|                 dt|                day|               hour|             minute|             second|target|\n",
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+\n",
      "|1.526085463008506E9|CvFEOw2nkgnSLToxZj|192.168.100.103|    43763.0|   23.248.75.75|  21130.0|  udp|missing|-999999.0| -999999.0| -999999.0|        S0|      D|      1.0|         40.0|      0.0|          0.0|   Benign|                NULL|2018-05-12 01:37:43|2018-05-12 00:00:00|2018-05-12 01:00:00|2018-05-12 01:37:00|2018-05-12 01:37:43|     0|\n",
      "|1.526085518009371E9| CxcjwELcHHk2bVjPl|192.168.100.103|    35341.0|    40.53.214.1|     23.0|  tcp|missing| 2.998789|       0.0|       0.0|        S0|      S|      3.0|        180.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-12 01:38:38|2018-05-12 00:00:00|2018-05-12 01:00:00|2018-05-12 01:38:00|2018-05-12 01:38:38|     1|\n",
      "|1.526085518018116E9|CTf3sS2qAm6oHxojW8|192.168.100.103|    57885.0|   158.13.85.42|   2323.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-12 01:38:38|2018-05-12 00:00:00|2018-05-12 01:00:00|2018-05-12 01:38:00|2018-05-12 01:38:38|     1|\n",
      "|1.526085463135682E9|Cb1g7V1g7gEkVne1ob|  209.239.24.41|       11.0|192.168.100.103|      0.0| icmp|missing|-999999.0| -999999.0| -999999.0|       OTH|missing|      1.0|         56.0|      0.0|          0.0|   Benign|                NULL|2018-05-12 01:37:43|2018-05-12 00:00:00|2018-05-12 01:00:00|2018-05-12 01:37:00|2018-05-12 01:37:43|     0|\n",
      "|1.526085464008435E9| CVNZWhdsDi613Mp97|192.168.100.103|    43763.0|  70.198.21.164|  20900.0|  udp|missing|-999999.0| -999999.0| -999999.0|        S0|      D|      1.0|         40.0|      0.0|          0.0|   Benign|                NULL|2018-05-12 01:37:44|2018-05-12 00:00:00|2018-05-12 01:00:00|2018-05-12 01:37:00|2018-05-12 01:37:44|     0|\n",
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"./processed.pq\").withColumn(\n",
    "    \"target\", F.when(F.col(\"label\") != \"Benign\", 1).otherwise(0)\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Data Preparation\n",
    "\n",
    "After loading our initial dataset, the next crucial step in our data engineering journey involves feature engineering. This process is essential for enhancing the performance of machine learning models by creating new features from the existing data.\n",
    "\n",
    "\n",
    "Once our data is adequately prepared, we will proceed to save the processed dataset. This step ensures that we can easily access our feature-engineered data for model training and evaluation, without needing to repeat these preprocessing steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime\n",
    "df = df.withColumn(\"timestamp\", from_unixtime(\"ts\"))\n",
    "# First, ensure 'dt' is a timestamp if not already\n",
    "df = df.withColumn(\"dt\", col(\"dt\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pyspark.sql.functions import col, unix_timestamp, from_unixtime, window\n",
    "\n",
    "def mins_to_secs(mins):\n",
    "    \"\"\"Convert minutes to seconds.\"\"\"\n",
    "    return mins * 60\n",
    "\n",
    "\n",
    "\n",
    "# Then, redefine generate_window function to accommodate timestamp operations correctly\n",
    "def generate_window(window_in_minutes: int, partition_by: str, timestamp_col: str):\n",
    "    return Window.partitionBy(partition_by) \\\n",
    "                 .orderBy(unix_timestamp(col(timestamp_col))) \\\n",
    "                 .rangeBetween(-mins_to_secs(window_in_minutes), 0)\n",
    "\n",
    "\n",
    "\n",
    "def generate_rolling_aggregate(col: str, partition_by: Optional[str] = None, operation: str = \"count\", \n",
    "                               timestamp_col: str = \"timestamp\", window_in_minutes: int = 1):\n",
    "    \"\"\"Generate rolling aggregate based on the specified operation.\"\"\"\n",
    "    window_spec = generate_window(window_in_minutes, partition_by if partition_by else col, timestamp_col)\n",
    "    operations = {\n",
    "        \"count\": F.count,\n",
    "        \"sum\": F.sum,\n",
    "        \"avg\": F.avg,\n",
    "        \"max\": F.max,\n",
    "        \"min\": F.min,\n",
    "        \"stddev\": F.stddev,\n",
    "        \"variance\": F.variance\n",
    "        # Extend with other operations as needed\n",
    "    }\n",
    "    \n",
    "    if operation in operations:\n",
    "        return operations[operation](col).over(window_spec)\n",
    "    else:\n",
    "        raise ValueError(f\"Operation {operation} is not supported. Supported operations: {list(operations.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumns({\n",
    "    \"source_ip_count_last_min\": generate_rolling_aggregate(col=\"source_ip\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"source_ip_count_last_30_mins\": generate_rolling_aggregate(col=\"source_ip\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"source_port_count_last_min\": generate_rolling_aggregate(col=\"source_port\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"source_port_count_last_30_mins\": generate_rolling_aggregate(col=\"source_port\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"dest_ip_count_last_min\": generate_rolling_aggregate(col=\"dest_ip\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"dest_ip_count_last_30_mins\": generate_rolling_aggregate(col=\"dest_ip\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"dest_port_count_last_min\": generate_rolling_aggregate(col=\"dest_port\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"dest_port_count_last_30_mins\": generate_rolling_aggregate(col=\"dest_port\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"source_ip_avg_pkts_last_min\": generate_rolling_aggregate(col=\"orig_pkts\", partition_by=\"source_ip\", operation=\"avg\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"source_ip_avg_pkts_last_30_mins\": generate_rolling_aggregate(col=\"orig_pkts\", partition_by=\"source_ip\", operation=\"avg\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"source_ip_avg_bytes_last_min\": generate_rolling_aggregate(col=\"orig_ip_bytes\", partition_by=\"source_ip\", operation=\"avg\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"source_ip_avg_bytes_last_30_mins\": generate_rolling_aggregate(col=\"orig_ip_bytes\", partition_by=\"source_ip\", operation=\"avg\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+------+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+-------------------+------------------------+----------------------------+--------------------------+------------------------------+----------------------+--------------------------+------------------------+----------------------------+---------------------------+-------------------------------+----------------------------+--------------------------------+\n",
      "|                 ts|               uid|    source_ip|source_port|        dest_ip|dest_port|proto|service| duration|orig_bytes|resp_bytes|conn_state|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes| label|detailed-label|                 dt|                day|               hour|             minute|             second|target|          timestamp|source_ip_count_last_min|source_ip_count_last_30_mins|source_port_count_last_min|source_port_count_last_30_mins|dest_ip_count_last_min|dest_ip_count_last_30_mins|dest_port_count_last_min|dest_port_count_last_30_mins|source_ip_avg_pkts_last_min|source_ip_avg_pkts_last_30_mins|source_ip_avg_bytes_last_min|source_ip_avg_bytes_last_30_mins|\n",
      "+-------------------+------------------+-------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+------+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+-------------------+------------------------+----------------------------+--------------------------+------------------------------+----------------------+--------------------------+------------------------+----------------------------+---------------------------+-------------------------------+----------------------------+--------------------------------+\n",
      "|1.525879850375819E9|C0kwyC47NorZVvNat8|122.152.1.198|       11.0|192.168.100.103|      0.0| icmp|missing|-999999.0| -999999.0| -999999.0|       OTH|missing|      1.0|         56.0|      0.0|          0.0|Benign|          NULL|2018-05-09 16:30:50|2018-05-09 00:00:00|2018-05-09 16:00:00|2018-05-09 16:30:00|2018-05-09 16:30:50|     0|2018-05-09 16:30:50|                       1|                           1|                         1|                             1|                     2|                         2|                       1|                           1|                        1.0|                            1.0|                        56.0|                            56.0|\n",
      "|1.525879941159259E9|CWIC0r1RKtp7gIu4j3|147.97.27.145|       11.0|192.168.100.103|      0.0| icmp|missing|-999999.0| -999999.0| -999999.0|       OTH|missing|      1.0|         56.0|      0.0|          0.0|Benign|          NULL|2018-05-09 16:32:21|2018-05-09 00:00:00|2018-05-09 16:00:00|2018-05-09 16:32:00|2018-05-09 16:32:21|     0|2018-05-09 16:32:21|                       1|                           1|                         1|                             2|                     4|                         8|                       1|                           2|                        1.0|                            1.0|                        56.0|                            56.0|\n",
      "|1.525880013173256E9|CvuA2k2lZGy3TUef96|204.115.183.3|       11.0|192.168.100.103|      0.0| icmp|missing|-999999.0| -999999.0| -999999.0|       OTH|missing|      1.0|         56.0|      0.0|          0.0|Benign|          NULL|2018-05-09 16:33:33|2018-05-09 00:00:00|2018-05-09 16:00:00|2018-05-09 16:33:00|2018-05-09 16:33:33|     0|2018-05-09 16:33:33|                       1|                           1|                         1|                             3|                     8|                        17|                       1|                           3|                        1.0|                            1.0|                        56.0|                            56.0|\n",
      "| 1.52588011003676E9| CBHbcC21IiYpMXLBF|   87.229.7.1|       11.0|192.168.100.103|      0.0| icmp|missing|-999999.0| -999999.0| -999999.0|       OTH|missing|      1.0|         56.0|      0.0|          0.0|Benign|          NULL|2018-05-09 16:35:10|2018-05-09 00:00:00|2018-05-09 16:00:00|2018-05-09 16:35:00|2018-05-09 16:35:10|     0|2018-05-09 16:35:10|                       1|                           1|                         1|                             4|                     6|                        26|                       1|                           4|                        1.0|                            1.0|                        56.0|                            56.0|\n",
      "|1.525880200281371E9| CQWrsQxegugGwOXB3| 58.229.90.26|       11.0|192.168.100.103|      0.0| icmp|missing|-999999.0| -999999.0| -999999.0|       OTH|missing|      1.0|         68.0|      0.0|          0.0|Benign|          NULL|2018-05-09 16:36:40|2018-05-09 00:00:00|2018-05-09 16:00:00|2018-05-09 16:36:00|2018-05-09 16:36:40|     0|2018-05-09 16:36:40|                       1|                           1|                         1|                             5|                     3|                        30|                       1|                           5|                        1.0|                            1.0|                        68.0|                            68.0|\n",
      "+-------------------+------------------+-------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+------+--------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+-------------------+------------------------+----------------------------+--------------------------+------------------------------+----------------------+--------------------------+------------------------+----------------------------+---------------------------+-------------------------------+----------------------------+--------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,execute and save the resulting table into a new parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"feature_engineered.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = spark.read.parquet(\"feature_engineered.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"source_ip_count_last_min\",\n",
    "    \"source_ip_count_last_30_mins\",\n",
    "    \"source_port_count_last_min\",\n",
    "    \"source_port_count_last_30_mins\",\n",
    "    \"source_ip_avg_pkts_last_min\",\n",
    "    \"source_ip_avg_pkts_last_30_mins\",\n",
    "    \"source_ip_avg_bytes_last_min\",\n",
    "    \"source_ip_avg_bytes_last_30_mins\",\n",
    "]\n",
    "categorical_features = [\"proto\", \"service\", \"conn_state\", \"history\"]\n",
    "categorical_features_indexed = [c + \"_index\" for c in categorical_features]\n",
    "\n",
    "input_features = numerical_features + categorical_features_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "# Assuming df is your DataFrame and categorical_features are defined\n",
    "for c in categorical_features:\n",
    "    # Identify frequent values for the feature 'c'\n",
    "    frequent_values = (\n",
    "        df.groupby(c)\n",
    "        .count()\n",
    "        .filter(col(\"count\") > 100)  # Threshold for considering a category as 'frequent'\n",
    "        .select(c)\n",
    "    )\n",
    "    \n",
    "    # Collect frequent values to a Python list\n",
    "    frequent_values_list = [row[c] for row in frequent_values.collect()]\n",
    "    \n",
    "    # Replace rare categories with 'Other'\n",
    "    df_c = df_c.withColumn(\n",
    "        c, \n",
    "        when(col(c).isin(frequent_values_list), col(c)).otherwise(lit(\"Other\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------+--------------------------+-----------------------+\n",
      "|count(DISTINCT proto)|count(DISTINCT service)|count(DISTINCT conn_state)|count(DISTINCT history)|\n",
      "+---------------------+-----------------------+--------------------------+-----------------------+\n",
      "|                    3|                      4|                         8|                     23|\n",
      "+---------------------+-----------------------+--------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_c.select([F.count_distinct(c) for c in categorical_features]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "Train test split will need to be done using the source IP address, otherwise we risk leaking data. The best way to do this is by splitting the IP addresses at random, and then filtering the data frame according to the IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|      source_ip|bad_sum|\n",
      "+---------------+-------+\n",
      "|192.168.100.103| 539473|\n",
      "|    192.168.2.5| 151566|\n",
      "|    192.168.2.1|      1|\n",
      "|  192.168.100.1|      0|\n",
      "|  219.250.49.64|      0|\n",
      "+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_c.groupby(\"source_ip\").agg(F.sum(F.col(\"target\")).alias(\"bad_sum\")).orderBy(\"bad_sum\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when\n",
    "\n",
    "# Assuming df_c is your original DataFrame\n",
    "# Define the IPs of interest\n",
    "malicious_ip_for_training = \"192.168.100.103\"\n",
    "malicious_ip_for_testing = \"192.168.2.5\"\n",
    "additional_ip_for_testing = \"192.168.2.1\"\n",
    "\n",
    "# Exclude these specific IPs from the initial sampling\n",
    "excluded_ips = [malicious_ip_for_training, malicious_ip_for_testing, additional_ip_for_testing]\n",
    "train_ips_sample = (\n",
    "    df_c.filter(~col(\"source_ip\").isin(excluded_ips))\n",
    "    .select(\"source_ip\")\n",
    "    .distinct()\n",
    "    .sample(False, 0.8, seed=42)  # Using sample with a seed for reproducibility\n",
    "    .withColumn(\"train_flag\", lit(1))  # Use a different column name for train flag\n",
    ")\n",
    "\n",
    "# Join the sampled IPs with a flag indicating training set membership\n",
    "df_c = df_c.join(train_ips_sample, on=\"source_ip\", how=\"left_outer\")\n",
    "\n",
    "# Resolve ambiguity by explicitly modifying 'train_flag'\n",
    "# Include the specific IPs in the training or testing set as needed\n",
    "df_c = df_c.withColumn(\"train_flag\",\n",
    "                       when(col(\"source_ip\") == malicious_ip_for_training, lit(1))\n",
    "                       .when(col(\"source_ip\").isin([malicious_ip_for_testing, additional_ip_for_testing]), lit(0))\n",
    "                       .otherwise(col(\"train_flag\"))\n",
    "                      )\n",
    "\n",
    "# Finally, create the train and test sets based on the 'train_flag'\n",
    "df_train = df_c.where((col(\"train_flag\") == 1) | (col(\"source_ip\") == malicious_ip_for_training))\n",
    "df_test = df_c.where((col(\"train_flag\").isNull() | (col(\"train_flag\") == 0)) | (col(\"source_ip\") == malicious_ip_for_testing) | (col(\"source_ip\") == additional_ip_for_testing))\n",
    "\n",
    "# Optionally, clean up by dropping the 'train_flag' column if it's no longer needed\n",
    "df_train = df_train.drop(\"train_flag\")\n",
    "df_test = df_test.drop(\"train_flag\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Machine Learning Pipline in PySpark\n",
    "\n",
    "This guide outlines the construction of a machine learning pipeline using PySpark, specifically for binary classification tasks. The pipeline integrates preprocessing, model training, and hyperparameter tuning stages tailored for datasets with both categorical and numerical features.\n",
    "\n",
    "## Pipeline Components\n",
    "\n",
    "1. **RandomForest Classifier**: \n",
    "   - `RandomForestClassifier` is chosen as the machine learning model, configured to predict a binary target variable. It's initialized with `numTrees=100`, indicating the ensemble will consist of 100 decision trees.\n",
    "\n",
    "2. **String Indexer**:\n",
    "   - `StringIndexer` is used to convert categorical features into numerical indices. \n",
    "\n",
    "3. **Vector Assembler**:\n",
    "   - The first `VectorAssembler` stage combines raw numerical features into a single vector, which is a required format for Spark's ML algorithms.\n",
    "\n",
    "4. **Standard Scaler**:\n",
    "   - `StandardScaler` standardizes features by removing the mean and scaling to unit variance. This is particularly important for algorithms sensitive to feature scale, but it's also a good practice to standardize data for general numerical stability.\n",
    "\n",
    "5. **Updated Vector Assembler**:\n",
    "   - A second `VectorAssembler` stage creates a final feature vector that combines the standardized numerical features and the indexed categorical features. This consolidated feature vector is then used for model training.\n",
    "\n",
    "## Hyperparameter Tuning and Model Evaluation\n",
    "\n",
    "- **Parameter Grid**:\n",
    "  - A `ParamGridBuilder` is used to define a grid of hyperparameters for tuning the RandomForest model. This example varies `numTrees` and `maxDepth` to find the best model configuration.\n",
    "  \n",
    "- **Cross Validator**:\n",
    "  - `CrossValidator` performs hyperparameter tuning and model selection. It evaluates different model configurations by cross-validation, using the parameter grid defined earlier. The process involves splitting the training data into a specified number of folds (here, `numFolds=3`), training the model on `n-1` folds, and evaluating it on the remaining fold. This cycle repeats for each combination of parameters in the grid to identify the best model.\n",
    "  \n",
    "- **Model Training**:\n",
    "  - The pipeline, including the RandomForest model and all preprocessing stages, is fit to the training data. The fitting process applies the preprocessing transformations and trains the model on the transformed data.\n",
    "\n",
    "- **Binary Classification Evaluator**:\n",
    "  - The performance of the best model found by cross-validation is evaluated using a `BinaryClassificationEvaluator`. This evaluator can compute various binary classification metrics, such as area under the ROC curve, to assess model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Define the stages of the pipeline\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"target\", numTrees=100)\n",
    "\n",
    "ind = StringIndexer(inputCols=categorical_features, outputCols=categorical_features_indexed, handleInvalid='keep')\n",
    "# Assemble numerical features and standardize\n",
    "va = VectorAssembler(inputCols=numerical_features, outputCol=\"numerical_features\")\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_numerical_features\", withStd=True, withMean=False)\n",
    "\n",
    "# Updated VectorAssembler to include standardized numerical features and indexed categorical features\n",
    "va2 = VectorAssembler(inputCols=[\"scaled_numerical_features\"] + categorical_features_indexed, outputCol=\"features\")\n",
    "\n",
    "# Define the stages of the pipeline including the scaler\n",
    "pipeline = Pipeline(stages=[ind, va, scaler, va2, rf])\n",
    "\n",
    "\n",
    "# Define a parameter grid for hyperparameter tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Define a cross-validator for hyperparameter tuning\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(labelCol=\"target\"),\n",
    "                          numFolds=3)\n",
    "# Fit the cross-validator to the training data\n",
    "cvModel = crossval.fit(df_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of trees: 100\n",
      "Best max depth: 10\n"
     ]
    }
   ],
   "source": [
    "# Access the best model from the cross-validation process\n",
    "bestModel = cvModel.bestModel\n",
    "\n",
    "# Assuming the RandomForestClassifier is the last stage in your pipeline,\n",
    "# you can access it like this:\n",
    "rfModel = bestModel.stages[-1]  # Adjust index based on your pipeline's structure\n",
    "\n",
    "# Now, print the hyperparameters of the RandomForest model\n",
    "# For example, to print the number of trees and max depth:\n",
    "print(f\"Best number of trees: {rfModel.getNumTrees}\")\n",
    "print(f\"Best max depth: {rfModel.getMaxDepth()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.956762461463573\n"
     ]
    }
   ],
   "source": [
    "# Predictions on the training data\n",
    "predictions = cvModel.transform(df_train)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"target\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>history</td>\n",
       "      <td>0.314674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>proto</td>\n",
       "      <td>0.233254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>source_port_count_last_30_mins</td>\n",
       "      <td>0.193774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>source_port_count_last_min</td>\n",
       "      <td>0.141115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>orig_ip_bytes</td>\n",
       "      <td>0.075309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>orig_pkts</td>\n",
       "      <td>0.016193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>duration</td>\n",
       "      <td>0.011925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resp_bytes</td>\n",
       "      <td>0.005079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>orig_bytes</td>\n",
       "      <td>0.003662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>conn_state</td>\n",
       "      <td>0.001763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>source_ip_count_last_min</td>\n",
       "      <td>0.001081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>source_ip_avg_pkts_last_min</td>\n",
       "      <td>0.000637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>source_ip_avg_pkts_last_30_mins</td>\n",
       "      <td>0.000538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>source_ip_count_last_30_mins</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>resp_pkts</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>source_ip_avg_bytes_last_min</td>\n",
       "      <td>0.000219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>resp_ip_bytes</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>source_ip_avg_bytes_last_30_mins</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>service</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Feature  Importance\n",
       "18                           history    0.314674\n",
       "15                             proto    0.233254\n",
       "10    source_port_count_last_30_mins    0.193774\n",
       "9         source_port_count_last_min    0.141115\n",
       "4                      orig_ip_bytes    0.075309\n",
       "3                          orig_pkts    0.016193\n",
       "0                           duration    0.011925\n",
       "2                         resp_bytes    0.005079\n",
       "1                         orig_bytes    0.003662\n",
       "17                        conn_state    0.001763\n",
       "7           source_ip_count_last_min    0.001081\n",
       "11       source_ip_avg_pkts_last_min    0.000637\n",
       "12   source_ip_avg_pkts_last_30_mins    0.000538\n",
       "8       source_ip_count_last_30_mins    0.000438\n",
       "5                          resp_pkts    0.000239\n",
       "13      source_ip_avg_bytes_last_min    0.000219\n",
       "6                      resp_ip_bytes    0.000057\n",
       "14  source_ip_avg_bytes_last_30_mins    0.000040\n",
       "16                           service    0.000003"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Feature Importance (assuming RandomForest is the last stage in the best model)\n",
    "\n",
    "# Access the best model from CrossValidator\n",
    "bestPipelineModel = cvModel.bestModel\n",
    "\n",
    "# Assuming RandomForest is the last stage in the pipeline\n",
    "rfModel = bestPipelineModel.stages[-1]\n",
    "\n",
    "# Feature names: This part is tricky because VectorAssembler doesn't store feature names\n",
    "# If you know the order and names of the features you can create this list manually\n",
    "# For demonstration, assuming we have a list of feature names\n",
    "featureNames = numerical_features + categorical_features  # Adjust accordingly\n",
    "\n",
    "# Retrieve feature importances\n",
    "importances = rfModel.featureImportances\n",
    "\n",
    "# Convert feature importances to a list, matching them with feature names\n",
    "# Note: This step assumes the direct correlation of feature indices with the names list\n",
    "importanceList = [(featureNames[i], importances[i]) for i in range(len(featureNames))]\n",
    "\n",
    "# Create a DataFrame from the list for better visualization and sort by importance\n",
    "importanceDF = pd.DataFrame(importanceList, columns=[\"Feature\", \"Importance\"]).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "importanceDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.8825\n",
      "PR AUC: 0.9819\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#  df_test is the test dataset and cvModel is the trained model\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_preds = cvModel.transform(df_test)\n",
    "\n",
    "# Evaluate ROC AUC and PR AUC\n",
    "roc_evaluator = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderROC\")\n",
    "pr_evaluator = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderPR\")\n",
    "\n",
    "roc_auc = roc_evaluator.evaluate(test_preds)\n",
    "pr_auc = pr_evaluator.evaluate(test_preds)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Model Performance in PySpark\n",
    "\n",
    "To enhance our machine learning pipeline and select the best model for our binary classification task, we aim to compare the performance of RandomForest with three other popular algorithms: CatBoost, XGBoost, and Logistic Regression. This comparison will provide insights into which model is most suitable for our dataset, considering various performance metrics.\n",
    "\n",
    "## Preparing the Data\n",
    "\n",
    "Ensure that the dataset is preprocessed appropriately, handling both categorical and numerical features to meet the requirements of each model. Consistent data preparation is crucial for a fair comparison.\n",
    "\n",
    "## Setting Up Models\n",
    "\n",
    "### RandomForest\n",
    "- Already implemented in the PySpark MLlib and part of the initial pipeline.\n",
    "\n",
    "### Logistic Regression\n",
    "- Use `pyspark.ml.classification.LogisticRegression`.\n",
    "- Can be seamlessly integrated into the Spark pipeline as a direct replacement for RandomForest.\n",
    "\n",
    "### XGBoost\n",
    "- Requires integrating the [Spark XGBoost](https://github.com/dmlc/xgboost/tree/master/jvm-packages) project.\n",
    "- Import `ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier` and configure similarly to RandomForest.\n",
    "\n",
    "### CatBoost\n",
    "- Does not have a native Spark integration.\n",
    "- Use local CatBoost training within a Spark UDF or the [CatBoost Spark package](https://github.com/catboost/catboost/tree/master/catboost/spark), if available.\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "Define parameter grids for each model, considering the unique hyperparameters relevant to each. Utilize `CrossValidator` for Spark models or model-specific tuning methods for external models like CatBoost and XGBoost.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Evaluate each model's performance using `BinaryClassificationEvaluator` or other relevant evaluators, focusing on metrics such as area under the ROC curve, accuracy, precision, and recall. This step is crucial for understanding each model's strengths and weaknesses.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"target\")\n",
    "\n",
    "# Pipeline with Logistic Regression instead of RandomForest\n",
    "pipeline_lr = Pipeline(stages=[ind, va, scaler, va2, lr])\n",
    "\n",
    "# Hyperparameter tuning and cross-validation setup could be similar to RandomForest\n",
    "\n",
    "\n",
    "lrModel  = pipeline_lr.fit(df_train)\n",
    "test_preds = lrModel.transform(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC 0.8698669049284795\n",
      "PR AUC 0.990320848399635\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "roc = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderROC\")\n",
    "print(\"ROC AUC\", roc.evaluate(test_preds))\n",
    "\n",
    "pr = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderPR\")\n",
    "print(\"PR AUC\", pr.evaluate(test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WARNING: Ignoring invalid distribution -yspark (c:\\\\users\\\\user\\\\anaconda3\\\\envs\\\\pyspark_env\\\\lib\\\\site-packages)',\n",
       " 'Collecting sparkxgb',\n",
       " '  Using cached sparkxgb-0.1-py3-none-any.whl',\n",
       " 'Collecting pyspark==3.1.1 (from sparkxgb)',\n",
       " '  Using cached pyspark-3.1.1-py2.py3-none-any.whl',\n",
       " 'Requirement already satisfied: py4j==0.10.9 in c:\\\\users\\\\user\\\\anaconda3\\\\envs\\\\pyspark_env\\\\lib\\\\site-packages (from pyspark==3.1.1->sparkxgb) (0.10.9)',\n",
       " 'WARNING: Ignoring invalid distribution -yspark (c:\\\\users\\\\user\\\\anaconda3\\\\envs\\\\pyspark_env\\\\lib\\\\site-packages)',\n",
       " 'Installing collected packages: pyspark, sparkxgb',\n",
       " 'Successfully installed pyspark-3.1.1 sparkxgb-0.1']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!pip install --user sparkxgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparkxgb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryClassificationEvaluator\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparkxgb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBoostClassifier\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create an XGBoostClassifier\u001b[39;00m\n\u001b[0;32m      6\u001b[0m xgb \u001b[38;5;241m=\u001b[39m XGBoostClassifier(\n\u001b[0;32m      7\u001b[0m     featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      8\u001b[0m     labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      9\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sparkxgb'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from sparkxgb import XGBoostClassifier\n",
    "\n",
    "# Create an XGBoostClassifier\n",
    "xgb = XGBoostClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"target\", \n",
    "    objective=\"binary:logistic\"\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[ind, va, xgb])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model = pipeline.fit(df_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_preds = model.transform(df_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(test_preds)\n",
    "print(\"ROC AUC:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparkxgb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIndexer, VectorAssembler\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparkxgb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBoostClassifier\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Define categorical features, numerical features, and indexed categorical features as you did before\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# ...\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create a feature vector assembler\u001b[39;00m\n\u001b[0;32m      9\u001b[0m va \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39minput_features, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, handleInvalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sparkxgb'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from sparkxgb import XGBoostClassifier\n",
    "\n",
    "# Define categorical features, numerical features, and indexed categorical features as you did before\n",
    "# ...\n",
    "\n",
    "# Create a feature vector assembler\n",
    "va = VectorAssembler(inputCols=input_features, outputCol=\"features\", handleInvalid='skip')\n",
    "\n",
    "# Create an XGBoostClassifier\n",
    "xgb = XGBoostClassifier(featuresCol=\"features\", labelCol=\"target\", numRound=100)\n",
    "\n",
    "# Create a pipeline with the stages\n",
    "pipeline = Pipeline(stages=[ind] + [va, xgb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmlspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmlspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBoostClassifier\n\u001b[0;32m      3\u001b[0m xgb \u001b[38;5;241m=\u001b[39m XGBoostClassifier(\n\u001b[0;32m      4\u001b[0m     featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      5\u001b[0m     labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      6\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Proceed with pipeline and cross-validation setup\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mmlspark'"
     ]
    }
   ],
   "source": [
    "from mmlspark.xgboost import XGBoostClassifier\n",
    "\n",
    "xgb = XGBoostClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"target\", \n",
    "    objective=\"binary:logistic\"\n",
    ")\n",
    "\n",
    "# Proceed with pipeline and cross-validation setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install catboost # optioanal install required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "# Define categorical features, numerical features, and indexed categorical features as you did before\n",
    "categorical_features = [\"proto\", \"service\", \"conn_state\", \"history\"]\n",
    "categorical_features_indexed = [c + \"_index\" for c in categorical_features]\n",
    "input_features = numerical_features + categorical_features_indexed\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "class CatBoostClassifierWrapper(Transformer, HasInputCol, HasOutputCol):\n",
    "    def __init__(self, model, inputCol, outputCol):\n",
    "        super(CatBoostClassifierWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        predict_udf = udf(lambda features: float(self.model.predict_proba([features.tolist()])[0][1]), DoubleType())\n",
    "        return dataset.withColumn(self.outputCol, predict_udf(dataset[self.inputCol]))\n",
    "\n",
    "# Usage in your pipeline\n",
    "catboost = CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='Logloss')\n",
    "\n",
    "catboost_wrapper = CatBoostClassifierWrapper(model=catboost, inputCol=\"features\", outputCol=\"prediction\")\n",
    "\n",
    "# Modify your pipeline stages\n",
    "pipeline = Pipeline(stages=[ind, va, catboost_wrapper])\n",
    "model = pipeline.fit(df_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC 0.8698650740921258\n",
      "PR AUC 0.9903206178941372\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "roc = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderROC\")\n",
    "print(\"ROC AUC\", roc.evaluate(test_preds))\n",
    "\n",
    "pr = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderPR\")\n",
    "print(\"PR AUC\", pr.evaluate(test_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
